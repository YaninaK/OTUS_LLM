{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Развёртка vLLM и локальное взаимодействие"
      ],
      "metadata": {
        "id": "SfR_6x-vgGLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install = False\n",
        "if install:\n",
        "  %pip install -q vllm\n",
        "  %pip install -q openai\n",
        "  %pip install -q mlflow"
      ],
      "metadata": {
        "id": "kDC6r6sckz7N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "import requests\n",
        "import json\n",
        "import openai\n",
        "import mlflow\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gXj5Wvy-GxU4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Развернуть vLLM с подходящий моделью"
      ],
      "metadata": {
        "id": "Jds9QSELGd-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_access_token=userdata.get('HF_token')\n",
        "login(token=hf_access_token)"
      ],
      "metadata": {
        "id": "_12_KTrbGrd_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ШАГ 1: Запуск vLLM ---\n",
        "print(\"--- ЗАПУСК vLLM SERVER ---\")\n",
        "vllm_cmd = [\n",
        "    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "    \"--model\", \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    \"--host\", \"0.0.0.0\",\n",
        "    \"--port\", \"8000\",\n",
        "    \"--gpu-memory-utilization\", \"0.6\",\n",
        "    \"--max-num-seqs\", \"8\",\n",
        "]\n",
        "\n",
        "vllm_process = subprocess.Popen(\n",
        "    vllm_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True\n",
        ")\n",
        "\n",
        "def log_vllm_output(pipe, prefix=\"\"):\n",
        "    for line in iter(pipe.readline, \"\"):\n",
        "        print(f\"[vLLM] {prefix}{line.strip()}\")\n",
        "    pipe.close()\n",
        "\n",
        "vllm_stdout_thread = threading.Thread(target=log_vllm_output, args=(vllm_process.stdout, \"OUT: \"))\n",
        "vllm_stderr_thread = threading.Thread(target=log_vllm_output, args=(vllm_process.stderr, \"ERR: \"))\n",
        "vllm_stdout_thread.start()\n",
        "vllm_stderr_thread.start()\n",
        "\n",
        "print(\"Ожидание запуска vLLM API сервера (ожидается 'Application startup complete.')...\")\n",
        "time.sleep(200) # Подождем, пока vLLM полностью не запустится\n",
        "print(\"--- vLLM SERVER ЗАПУЩЕН (предположительно) ---\")\n",
        "\n",
        "# --- ШАГ 2: Установка и запуск localtunnel ---\n",
        "print(\"\\n--- УСТАНОВКА И ЗАПУСК LOCALTUNNEL ---\")\n",
        "try:\n",
        "    subprocess.run([\"npm\", \"install\", \"-g\", \"localtunnel\"], check=True)\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"Ошибка установки localtunnel. Проверьте npm.\")\n",
        "    exit()\n",
        "\n",
        "lt_cmd = [\"lt\", \"--port\", \"8000\"]\n",
        "lt_process = subprocess.Popen(lt_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "\n",
        "def log_lt_output(pipe, prefix=\"\"):\n",
        "    for line in iter(pipe.readline, \"\"):\n",
        "        print(f\"[LT] {prefix}{line.strip()}\")\n",
        "        # Пытаемся найти URL в выводе stderr\n",
        "        if \"your url is:\" in line:\n",
        "            global found_url\n",
        "            found_url = line.split(\"your url is:\")[1].strip()\n",
        "            print(f\"\\n### TUNNEL URL НАЙДЕН: {found_url} ###\")\n",
        "            print(\"### КОПИРУЕМ ЭТОТ URL ДЛЯ СЛЕДУЮЩЕГО ШАГА ###\")\n",
        "    pipe.close()\n",
        "\n",
        "global found_url\n",
        "found_url = None\n",
        "\n",
        "lt_stdout_thread = threading.Thread(target=log_lt_output, args=(lt_process.stdout, \"OUT: \"))\n",
        "lt_stderr_thread = threading.Thread(target=log_lt_output, args=(lt_process.stderr, \"ERR: \"))\n",
        "lt_stdout_thread.start()\n",
        "lt_stderr_thread.start()\n",
        "\n",
        "print(\"Ожидание создания туннеля localtunnel...\")\n",
        "# Ждем несколько секунд, чтобы localtunnel успел создать соединение и вывести URL\n",
        "time.sleep(20)\n",
        "\n",
        "if found_url:\n",
        "    print(f\"\\n*** ГОТОВО ***\")\n",
        "    print(f\"Наш API URL: {found_url}/v1\")\n",
        "    print(\"Копируем его и используем в следующей ячейке.\")\n",
        "else:\n",
        "    print(\"\\n--- Предупреждение ---\")\n",
        "    print(\"URL туннеля не был найден автоматически в выводе.\")\n",
        "    print(\"Проверить вывод ячейки выше на наличие строки 'your url is: ...'\")\n",
        "    print(\"Если URL не появился, возможно, localtunnel завис или не смог подключиться.\")\n",
        "\n",
        "# Теперь процесс vLLM и localtunnel работают в фоне.\n",
        "# Ячейка завершена, но процессы остаются активными."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smRwrReashs4",
        "outputId": "e8e86c7c-9c18-4473-ef75-f572b5cbcf91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ЗАПУСК vLLM SERVER ---\n",
            "Ожидание запуска vLLM API сервера (ожидается 'Application startup complete.')...\n",
            "[vLLM] ERR: 2026-01-26 18:07:16.480477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM] ERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM] ERR: E0000 00:00:1769450836.523656    4756 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM] ERR: E0000 00:00:1769450836.535383    4756 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM] ERR: W0000 00:00:1769450836.562670    4756 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450836.562812    4756 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450836.562820    4756 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450836.562823    4756 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: 2026-01-26 18:07:16.570179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "[vLLM] ERR: To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:27 [api_server.py:1272] vLLM API server version 0.14.1\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:27 [utils.py:263] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-0.5B-Instruct', 'gpu_memory_utilization': 0.6, 'max_num_seqs': 8}\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:28 [model.py:530] Resolved architecture: Qwen2ForCausalLM\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m WARNING 01-26 18:07:28 [model.py:1817] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m WARNING 01-26 18:07:28 [model.py:1869] Casting torch.bfloat16 to torch.float16.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:28 [model.py:1545] Using max model len 32768\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:28 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:28 [vllm.py:630] Asynchronous scheduling is enabled.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:07:28 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
            "[vLLM] ERR: 2026-01-26 18:07:36.102964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM] ERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM] ERR: E0000 00:00:1769450856.137704    4869 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM] ERR: E0000 00:00:1769450856.145385    4869 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM] ERR: W0000 00:00:1769450856.161524    4869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450856.161558    4869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450856.161561    4869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: W0000 00:00:1769450856.161564    4869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] ERR: AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:46 [core.py:97] Initializing a V1 LLM engine (v0.14.1) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 16, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:48 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.28.0.12:54483 backend=nccl\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:48 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:49 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m ERROR 01-26 18:07:49 [fa_utils.py:82] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:50 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:51 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] ERR: Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] ERR: Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] ERR: Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:52 [default_loader.py:291] Loading weights took 1.49 seconds\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:07:54 [gpu_model_runner.py:3905] Model loading took 0.93 GiB memory and 3.261697 seconds\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:00 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca3168cf87/rank_0_0/backbone for vLLM's torch.compile\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:00 [backends.py:704] Dynamo bytecode transform time: 6.20 s\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:07 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 0.851 s\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:07 [monitor.py:34] torch.compile takes 7.05 s in total\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:08 [gpu_worker.py:358] Available KV cache memory: 7.81 GiB\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:09 [kv_cache_utils.py:1305] GPU KV cache size: 682,400 tokens\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:09 [kv_cache_utils.py:1310] Maximum concurrency for 32,768 tokens per request: 20.83x\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:09 [kernel_warmup.py:64] Warming up FlashInfer attention.\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] ERR: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "[vLLM] ERR: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 25.39it/s]\n",
            "[vLLM] ERR: Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 21.78it/s]\n",
            "[vLLM] ERR: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m\n",
            "[vLLM] ERR: Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "[vLLM] ERR: Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 22.95it/s]\n",
            "[vLLM] ERR: Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 22.91it/s]\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:11 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.09 GiB\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:11 [core.py:273] init engine (profile, create kv cache, warmup model) took 17.17 seconds\n",
            "[vLLM] OUT: \u001b[0;36m(EngineCore_DP0 pid=4869)\u001b[0;0m INFO 01-26 18:08:13 [vllm.py:630] Asynchronous scheduling is enabled.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:13 [api_server.py:1014] Supported tasks: ['generate']\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m WARNING 01-26 18:08:14 [model.py:1358] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:14 [serving_responses.py:224] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:14 [serving_chat.py:146] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:14 [serving_chat.py:182] Warming up chat template processing...\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:17 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:17 [serving_chat.py:218] Chat template warmup completed in 2906.9ms\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:17 [serving_completion.py:78] Using default completion sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [serving_chat.py:146] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [api_server.py:1346] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:38] Available routes are:\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /docs, Methods: GET, HEAD\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /redoc, Methods: GET, HEAD\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /tokenize, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /detokenize, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /inference/v1/generate, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /pause, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /resume, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /is_paused, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /metrics, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /health, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /load, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/models, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /version, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/responses, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/messages, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/chat/completions, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/completions, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/audio/translations, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /ping, Methods: GET\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /ping, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /invocations, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /classify, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/embeddings, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /score, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/score, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /rerank, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v1/rerank, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /v2/rerank, Methods: POST\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:08:18 [launcher.py:46] Route: /pooling, Methods: POST\n",
            "[vLLM] ERR: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     Started server process [4756]\n",
            "[vLLM] ERR: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     Waiting for application startup.\n",
            "[vLLM] ERR: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     Application startup complete.\n",
            "--- vLLM SERVER ЗАПУЩЕН (предположительно) ---\n",
            "\n",
            "--- УСТАНОВКА И ЗАПУСК LOCALTUNNEL ---\n",
            "Ожидание создания туннеля localtunnel...\n",
            "[LT] OUT: your url is: https://tired-mammals-poke.loca.lt\n",
            "\n",
            "### TUNNEL URL НАЙДЕН: https://tired-mammals-poke.loca.lt ###\n",
            "### КОПИРУЕМ ЭТОТ URL ДЛЯ СЛЕДУЮЩЕГО ШАГА ###\n",
            "\n",
            "*** ГОТОВО ***\n",
            "Наш API URL: https://tired-mammals-poke.loca.lt/v1\n",
            "Копируем его и используем в следующей ячейке.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Взаимодействие с локальной моделью\n",
        "\n",
        "- Используя библиотеку для http запросов (httpx / requests / etc) обратитесь к модели и получите ответ (например с вопросом \"What is the capital of Germany?\").\n",
        "- Сделайте тоже самое используя библиотеку openai (тут придется столкнуться с совместимостью OpenAI API и разных моделей, будьте внимательны).\n"
      ],
      "metadata": {
        "id": "iJ9rdoUWJH9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Библиотека для http запросов"
      ],
      "metadata": {
        "id": "4SZp4jH9OLLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ВСТАВИМ СВОЙ URL ОТ LOCALTUNNEL (без /v1 на конце) ---\n",
        "OUR_LT_URL = \"https://tired-mammals-poke.loca.lt\" # <- НАШ РЕАЛЬНЫЙ URL ОТ LOCALTUNNEL\n",
        "\n",
        "# Формируем правильный API_URL, добавляя /v1\n",
        "API_URL = f\"{OUR_LT_URL.rstrip('/')}/v1\" # rstrip убирает лишний / на конце, если он есть\n",
        "\n",
        "print(f\"Используем API_URL: {API_URL}\")\n",
        "\n",
        "url = f\"{API_URL}/chat/completions\" # Теперь это будет OUR_LT_URL/v1/chat/completions\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 150\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(f\"Отправляю запрос к {url}\")\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    response.raise_for_status()\n",
        "\n",
        "    result = response.json()\n",
        "    # print(json.dumps(result, indent=2)) # Для просмотра полного ответа\n",
        "\n",
        "    answer = result['choices'][0]['message']['content'].strip()\n",
        "    print(f\"\\n--- Ответ модели ---\\n{answer}\\n-------------------\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Произошла ошибка при запросе: {e}\")\n",
        "    if response is not None:\n",
        "        print(f\"Статус код: {response.status_code}\")\n",
        "        print(f\"Тело ответа: {response.text}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Ошибка при парсинге ответа: {e}\")\n",
        "    print(f\"Полученный JSON: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZwJZsE9wGAO",
        "outputId": "3711bd6f-db64-4cac-cd9d-2186a1e674ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используем API_URL: https://tired-mammals-poke.loca.lt/v1\n",
            "Отправляю запрос к https://tired-mammals-poke.loca.lt/v1/chat/completions\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\n",
            "--- Ответ модели ---\n",
            "The capital of Germany is Berlin.\n",
            "-------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Библиотека openai"
      ],
      "metadata": {
        "id": "RmjmMNU5OBdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Настраиваем клиент OpenAI для работы с vLLM\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"{OUR_LT_URL.rstrip('/')}/v1\",\n",
        "    api_key=\"not-needed\"  # Для vLLM API ключ не требуется\n",
        ")\n",
        "\n",
        "# Создаем запрос\n",
        "try:\n",
        "    print(f\"Отправляю запрос к {OUR_LT_URL}/v1\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Ответ модели ---\")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(f\"-------------------\")\n",
        "\n",
        "    # Дополнительная информация о запросе (опционально)\n",
        "    print(f\"\\nДополнительная информация:\")\n",
        "    print(f\"Модель: {response.model}\")\n",
        "    print(f\"Использовано токенов: {response.usage.total_tokens}\")\n",
        "\n",
        "except openai.APIError as e:\n",
        "    print(f\"Ошибка API: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла ошибка: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0m1O77YNQVE",
        "outputId": "729e16a3-0c7d-4451-c4be-548706d1ec15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Отправляю запрос к https://tired-mammals-poke.loca.lt/v1\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\n",
            "--- Ответ модели ---\n",
            "The capital of Germany is Berlin.\n",
            "-------------------\n",
            "\n",
            "Дополнительная информация:\n",
            "Модель: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Использовано токенов: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mlflow и нтеграция кастомной genai метрики:\n",
        "\n",
        "- Создать кастомную метрику для оценки эксперимента используя локальную модель.\n",
        "- Прогнать эксперимент на любом датафрейме с использованием локальной модели для расчета метрики."
      ],
      "metadata": {
        "id": "BO4xuo5R27L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Укажите путь к вашему хранилищу\n",
        "mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
        "\n",
        "# Получить информацию о последнем запуске в эксперименте\n",
        "experiment_name = \"Real_Scenario_Local_LLMEval\"\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjquIPXz7Rwx",
        "outputId": "c0add611-6a4f-4415-c414-f4abb26fa9f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "2026/01/26 18:12:29 INFO mlflow.tracking.fluent: Experiment with name 'Real_Scenario_Local_LLMEval' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/mlruns/235368573044722462', creation_time=1769451149541, experiment_id='235368573044722462', last_update_time=1769451149541, lifecycle_stage='active', name='Real_Scenario_Local_LLMEval', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- НАСТРОЙКИ ---\n",
        "OUR_API_BASE_URL = OUR_LT_URL # <- НАШ РЕАЛЬНЫЙ URL ОТ ТУННЕЛЯ (БЕЗ /v1)\n",
        "MODEL_NAME_FOR_EVAL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# -----------------\n",
        "\n",
        "def generate_answer(question, context=None):\n",
        "    \"\"\"\n",
        "    Генерирует ответ на вопрос с помощью локальной LLM (через API).\n",
        "    \"\"\"\n",
        "    api_url = f\"{OUR_API_BASE_URL.rstrip('/')}/v1/chat/completions\"\n",
        "\n",
        "    context_str = context if context else \"\"\n",
        "    prompt = f\"{context_str}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME_FOR_EVAL,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.1, # Для более детерминированного ответа\n",
        "        \"max_tokens\": 100,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, headers={\"Content-Type\": \"application/json\"}, json=payload)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        answer = result['choices'][0]['message']['content'].strip()\n",
        "        return answer\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Ошибка при генерации для вопроса '{question[:30]}...': {e}\")\n",
        "        return \"Error generating answer\"\n",
        "    except Exception as e:\n",
        "        print(f\"Неожиданная ошибка при генерации для вопроса '{question[:30]}...': {e}\")\n",
        "        return \"Error generating answer\"\n"
      ],
      "metadata": {
        "id": "-3VSLwtq139d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prediction_quality(predictions, contexts=None, references=None):\n",
        "    \"\"\"\n",
        "    Оценивает качество предсказаний с помощью локальной LLM.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    api_url = f\"{OUR_API_BASE_URL.rstrip('/')}/v1/chat/completions\"\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        context_str = contexts[i] if contexts else \"Контекст не предоставлен.\"\n",
        "        reference_str = references[i] if references else \"Эталонный ответ не предоставлен.\"\n",
        "\n",
        "        evaluation_prompt = f\"\"\"\n",
        "        Задание: Оцените качество следующего предсказанного ответа по шкале от 0 до 10.\n",
        "        Контекст: {context_str}\n",
        "        Вопрос: (можно вывести из контекста или подразумевается)\n",
        "        Предсказанный ответ: {pred}\n",
        "        Эталонный ответ: {reference_str}\n",
        "\n",
        "        Обоснуйте оценку кратко, а затем поставьте оценку в формате \"Оценка: X\", где X - число от 0 до 10.\n",
        "        \"\"\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": MODEL_NAME_FOR_EVAL,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
        "            \"temperature\": 0.0, # Для детерминированности\n",
        "            \"max_tokens\": 100,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, headers={\"Content-Type\": \"application/json\"}, json=payload)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            full_response_text = result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "            import re\n",
        "            match = re.search(r'Оценка:\\s*(\\d+\\.?\\d*)', full_response_text)\n",
        "            if match:\n",
        "                score = float(match.group(1))\n",
        "                score = max(0.0, min(10.0, score))\n",
        "            else:\n",
        "                print(f\"Предупреждение: Не удалось извлечь оценку из ответа для предсказания {i}: {full_response_text}\")\n",
        "                score = 0.0\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Ошибка при оценке для предсказания {i}: {e}\")\n",
        "            score = 0.0\n",
        "        except Exception as e:\n",
        "            print(f\"Неожиданная ошибка при оценке {i}: {e}\")\n",
        "            score = 0.0\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "FAq0B0ot15rh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Подготовка тестовых данных ---\n",
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who wrote 'Romeo and Juliet'?\",\n",
        "    \"What is the boiling point of water?\",\n",
        "    \"How many planets are in our solar system?\",\n",
        "    \"What is the largest mammal?\",\n",
        "    \"What is the chemical symbol for gold?\",\n",
        "    \"Who painted the Mona Lisa?\",\n",
        "    \"What is the tallest mountain in the world?\",\n",
        "    \"What is the currency of Japan?\",\n",
        "    \"What is the fastest land animal?\",\n",
        "]\n",
        "\n",
        "contexts = [\n",
        "    \"Simple geography question.\",\n",
        "    \"Simple literature question.\",\n",
        "    \"Simple science question.\",\n",
        "    \"Simple astronomy question.\",\n",
        "    \"Simple biology question.\",\n",
        "    \"Simple chemistry question.\",\n",
        "    \"Simple art question.\",\n",
        "    \"Simple geography question.\",\n",
        "    \"Simple economics question.\",\n",
        "    \"Simple biology question.\",\n",
        "]\n",
        "\n",
        "# Истинные (эталонные) ответы\n",
        "true_references = [\n",
        "    \"Paris\",\n",
        "    \"William Shakespeare\",\n",
        "    \"100°C at sea level\",\n",
        "    \"Eight\",\n",
        "    \"The blue whale is the largest mammal.\",\n",
        "    \"Au\",\n",
        "    \"Leonardo da Vinci\",\n",
        "    \"Mount Everest\",\n",
        "    \"Yen\",\n",
        "    \"The cheetah is the fastest land animal.\",\n",
        "]\n",
        "\n",
        "# --- Генерация предсказаний основной моделью ---\n",
        "print(\"Начинаю генерацию предсказаний основной моделью...\")\n",
        "generated_predictions = []\n",
        "for q in questions:\n",
        "    ans = generate_answer(q)\n",
        "    generated_predictions.append(ans)\n",
        "    print(f\"Вопрос: {q}\")\n",
        "    print(f\"Предсказание: {ans}\\n---\")\n",
        "\n",
        "# --- Подготовка данных для оценки ---\n",
        "df_for_evaluation = pd.DataFrame({\n",
        "    \"question\": questions,\n",
        "    \"context\": contexts,\n",
        "    \"prediction\": generated_predictions, # <-- Сгенерировано моделью\n",
        "    \"reference\": true_references,       # <-- Истина\n",
        "})\n",
        "\n",
        "print(\"\\n--- Подготовленные данные для оценки ---\")\n",
        "print(df_for_evaluation[['question', 'prediction', 'reference']].head()) # Показываем первые 5\n",
        "\n",
        "# --- Запуск MLflow эксперимента ---\n",
        "experiment_name = \"Real_Scenario_Local_LLMEval\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    RUN_ID = run.info.run_id\n",
        "    # Логирование параметров\n",
        "    mlflow.log_param(\"model_for_generation\", MODEL_NAME_FOR_EVAL)\n",
        "    mlflow.log_param(\"model_for_evaluation\", MODEL_NAME_FOR_EVAL)\n",
        "    mlflow.log_param(\"evaluation_api_base\", OUR_API_BASE_URL)\n",
        "    mlflow.log_param(\"num_samples\", len(questions))\n",
        "\n",
        "    # Вычисление кастомной метрики\n",
        "    print(\"\\nНачинаю вычисление кастомной метрики...\")\n",
        "    eval_scores = evaluate_prediction_quality(\n",
        "        df_for_evaluation['prediction'].tolist(),\n",
        "        df_for_evaluation['context'].tolist(), # contexts могут быть полезны для оценки\n",
        "        df_for_evaluation['reference'].tolist()\n",
        "    )\n",
        "\n",
        "    mean_eval_score = sum(eval_scores) / len(eval_scores) if eval_scores else 0.0\n",
        "\n",
        "    # Логирование результатов\n",
        "    mlflow.log_metric(\"custom_llm_eval_score_mean\", mean_eval_score)\n",
        "    # Логирование индивидуальных оценок (опционально)\n",
        "    for i, score in enumerate(eval_scores):\n",
        "        mlflow.log_metric(f\"custom_llm_eval_score_item_{i}\", score)\n",
        "\n",
        "    # Логирование данных (опционально, для анализа)\n",
        "    # df_for_evaluation.to_csv(\"eval_data.csv\", index=False)\n",
        "    # mlflow.log_artifact(\"eval_data.csv\")\n",
        "\n",
        "    print(f\"\\nЭксперимент завершён.\")\n",
        "    print(f\"Средняя оценка (кастомная метрика): {mean_eval_score:.2f}\")\n",
        "    print(f\"Данные и результаты доступны в MLflow UI.\")\n",
        "\n",
        "print(\"\\nПроверьте MLflow UI для просмотра полных результатов эксперимента.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3KVvVx51-SH",
        "outputId": "1af5f52a-8be2-4184-b6c9-cba2d90244d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начинаю генерацию предсказаний основной моделью...\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the capital of France?\n",
            "Предсказание: The capital of France is Paris.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: Who wrote 'Romeo and Juliet'?\n",
            "Предсказание: \"Romeo and Juliet\" was written by William Shakespeare. This play is one of the most famous tragedies in Western literature and has been performed countless times across different languages and cultures. The story revolves around two young lovers who fall in love with each other but are rejected by their families due to their gender differences. The play explores themes such as love, revenge, and the consequences of societal expectations on individuals. It has had a lasting impact on literature and continues to be studied and appreciated today.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:12:58 [loggers.py:257] Engine 000: Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 12.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 40.8%\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the boiling point of water?\n",
            "Предсказание: The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure. This temperature is known as the melting point and is used to determine when water becomes liquid. At this temperature, water molecules gain enough energy to overcome the intermolecular forces holding them together, causing them to boil and turn into steam. The exact boiling point can vary slightly depending on factors such as altitude, humidity, and atmospheric pressure.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: How many planets are in our solar system?\n",
            "Предсказание: There are eight planets in our solar system:\n",
            "\n",
            "1. Mercury\n",
            "2. Venus\n",
            "3. Earth\n",
            "4. Mars\n",
            "5. Jupiter\n",
            "6. Saturn\n",
            "7. Uranus\n",
            "8. Neptune\n",
            "\n",
            "These planets orbit the Sun in order from the innermost to the outermost, with Mercury at the very bottom and Neptune at the top. Each planet has its own unique characteristics, such as size, mass, composition, and orbital period around the Sun.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the largest mammal?\n",
            "Предсказание: The largest mammal is the blue whale (Balaenoptera musculus). Blue whales can grow up to 100 feet in length and weigh over 250 tons. They are known for their massive size and powerful heart, which allows them to pump blood at high speeds through their long, narrow bodies. These animals play crucial roles in marine ecosystems as top predators and provide food for many species of fish and other marine life.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the chemical symbol for gold?\n",
            "Предсказание: The chemical symbol for gold is Au (pronounced \"A-U\"). Gold is an element with the atomic number 79 and is a precious metal that has been used in jewelry, electronics, and other applications since ancient times.\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: Who painted the Mona Lisa?\n",
            "Предсказание: The Mona Lisa was painted by Leonardo da Vinci. This iconic painting is considered one of the most famous works in the world and has captivated audiences for over 400 years. The painting depicts an enigmatic figure with almond-shaped eyes, a smile that seems to say \"I am not here,\" and a mysterious expression that leaves viewers in awe. It is believed that Leonardo da Vinci completed the painting around 1503-1506, though some sources suggest it may have\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the tallest mountain in the world?\n",
            "Предсказание: The tallest mountain in the world is Mount Everest, which stands at an elevation of 8,848 meters (29,029 feet) above sea level. This mountain is located in the Himalayas and is part of the region known as the \"Tibetan Plateau.\" It's also one of the highest peaks on Earth, with its summit being the highest point on the planet. The mountain has been climbed by many climbers over the years, including Sherpas who have\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the currency of Japan?\n",
            "Предсказание: The currency of Japan is the Japanese yen (JPY).\n",
            "---\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:13:08 [loggers.py:257] Engine 000: Avg prompt throughput: 24.4 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 40.0%\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Вопрос: What is the fastest land animal?\n",
            "Предсказание: The fastest land animal is the cheetah. Cheetahs can reach speeds of up to 76 miles per hour (122 kilometers per hour) in short bursts, making them one of the fastest land animals on Earth. They have adapted their body structure and physiology to allow for such high-speed running, which has allowed them to dominate various environments over millions of years.\n",
            "---\n",
            "\n",
            "--- Подготовленные данные для оценки ---\n",
            "                                    question  \\\n",
            "0             What is the capital of France?   \n",
            "1              Who wrote 'Romeo and Juliet'?   \n",
            "2        What is the boiling point of water?   \n",
            "3  How many planets are in our solar system?   \n",
            "4                What is the largest mammal?   \n",
            "\n",
            "                                          prediction  \\\n",
            "0                    The capital of France is Paris.   \n",
            "1  \"Romeo and Juliet\" was written by William Shak...   \n",
            "2  The boiling point of water is 100 degrees Cels...   \n",
            "3  There are eight planets in our solar system:\\n...   \n",
            "4  The largest mammal is the blue whale (Balaenop...   \n",
            "\n",
            "                               reference  \n",
            "0                                  Paris  \n",
            "1                    William Shakespeare  \n",
            "2                     100°C at sea level  \n",
            "3                                  Eight  \n",
            "4  The blue whale is the largest mammal.  \n",
            "\n",
            "Начинаю вычисление кастомной метрики...\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Предупреждение: Не удалось извлечь оценку из ответа для предсказания 0: К сожалению, у меня нет доступа к конкретному вопросу или контексту, который мог бы помочь мне оценить качество предсказанного ответа. Однако я могу предоставить вам общую оценку, если вы предоставите конкретное предложение или вопрос, которое вы хотите оценивать.\n",
            "\n",
            "Если у вас есть конкретная строка или вопрос, который вы хотите оценивать, пожалуйста, дайте мне\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO 01-26 18:13:18 [loggers.py:257] Engine 000: Avg prompt throughput: 138.7 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 24.5%\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "[vLLM] OUT: \u001b[0;36m(APIServer pid=4756)\u001b[0;0m INFO:     35.240.191.247:0 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\n",
            "Эксперимент завершён.\n",
            "Средняя оценка (кастомная метрика): 7.90\n",
            "Данные и результаты доступны в MLflow UI.\n",
            "\n",
            "Проверьте MLflow UI для просмотра полных результатов эксперимента.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/mlruns/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8kzO4BTPevL",
        "outputId": "25c3a0ab-7b63-4f71-efea-92394e820750"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlruns/:\n",
            "0  235368573044722462\n",
            "\n",
            "/content/mlruns/0:\n",
            "meta.yaml\n",
            "\n",
            "/content/mlruns/235368573044722462:\n",
            "2bc4c01f147d4e91a174a56c2d4b5acb  meta.yaml\n",
            "\n",
            "/content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb:\n",
            "artifacts  meta.yaml  metrics  params  tags\n",
            "\n",
            "/content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb/artifacts:\n",
            "\n",
            "/content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb/metrics:\n",
            "custom_llm_eval_score_item_0  custom_llm_eval_score_item_6\n",
            "custom_llm_eval_score_item_1  custom_llm_eval_score_item_7\n",
            "custom_llm_eval_score_item_2  custom_llm_eval_score_item_8\n",
            "custom_llm_eval_score_item_3  custom_llm_eval_score_item_9\n",
            "custom_llm_eval_score_item_4  custom_llm_eval_score_mean\n",
            "custom_llm_eval_score_item_5\n",
            "\n",
            "/content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb/params:\n",
            "evaluation_api_base  model_for_evaluation  model_for_generation  num_samples\n",
            "\n",
            "/content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb/tags:\n",
            "mlflow.runName\tmlflow.source.name  mlflow.source.type\tmlflow.user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_ID = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
        "!cat /content/mlruns/{EXPERIMENT_ID}/{RUN_ID}/meta.yaml\n",
        "!ls /content/mlruns/{EXPERIMENT_ID}/{RUN_ID}/metrics\n",
        "!ls /content/mlruns/{EXPERIMENT_ID}/{RUN_ID}/params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocBwIQurQD0l",
        "outputId": "0a838f02-a3d8-4eb4-edb4-a0e037ce76d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artifact_uri: file:///content/mlruns/235368573044722462/2bc4c01f147d4e91a174a56c2d4b5acb/artifacts\n",
            "end_time: 1769451204318\n",
            "entry_point_name: ''\n",
            "experiment_id: '235368573044722462'\n",
            "lifecycle_stage: active\n",
            "run_id: 2bc4c01f147d4e91a174a56c2d4b5acb\n",
            "run_name: rebellious-horse-690\n",
            "source_name: ''\n",
            "source_type: 4\n",
            "source_version: ''\n",
            "start_time: 1769451190227\n",
            "status: 3\n",
            "tags: []\n",
            "user_id: root\n",
            "custom_llm_eval_score_item_0  custom_llm_eval_score_item_6\n",
            "custom_llm_eval_score_item_1  custom_llm_eval_score_item_7\n",
            "custom_llm_eval_score_item_2  custom_llm_eval_score_item_8\n",
            "custom_llm_eval_score_item_3  custom_llm_eval_score_item_9\n",
            "custom_llm_eval_score_item_4  custom_llm_eval_score_mean\n",
            "custom_llm_eval_score_item_5\n",
            "evaluation_api_base  model_for_evaluation  model_for_generation  num_samples\n"
          ]
        }
      ]
    }
  ]
}